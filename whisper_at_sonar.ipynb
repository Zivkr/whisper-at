{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "! pip install \"whisper-at@git+https://github.com/Zivkr/whisper-at.git#subdirectory=package/whisper-at\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import whisper_at as whisper\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T10:10:10.216413667Z",
     "start_time": "2023-08-20T10:10:07.738530428Z"
    }
   },
   "id": "24971c69601567fb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# download a sample audio\n",
    "# !pip -q install wget\n",
    "# import wget,IPython\n",
    "# wget.download('https://www.dropbox.com/s/7eznyazmc1pmw9h/case_closed.wav?dl=1', '/home/zivk/Documents/Deep Learning/whisper-at/sample_audio.flac')\n",
    "# IPython.display.Audio('/home/zivk/Documents/Deep Learning/whisper-at/sample_audio.flac')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T09:58:36.659512846Z",
     "start_time": "2023-08-20T09:58:36.635479046Z"
    }
   },
   "id": "7e7e7c85bef08611"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper(\n",
      "  (encoder): AudioEncoder(\n",
      "    (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TextDecoder(\n",
      "    (token_embedding): Embedding(51865, 384)\n",
      "    (blocks): ModuleList(\n",
      "      (0-3): 4 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (cross_attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (at_model): ATModel(\n",
      "    (time_tr): ResidualAttentionBlock(\n",
      "      (attn): MultiHeadAttention(\n",
      "        (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (out): Linear(in_features=384, out_features=384, bias=True)\n",
      "      )\n",
      "      (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      )\n",
      "      (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (layer_tr): ResidualAttentionBlock(\n",
      "      (attn): MultiHeadAttention(\n",
      "        (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (out): Linear(in_features=384, out_features=384, bias=True)\n",
      "      )\n",
      "      (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      )\n",
      "      (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (mlp_layer): Identity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "audio_tagging_time_resolution = 10\n",
    "model = whisper.load_model(\"tiny\")\n",
    "\n",
    "# Freeze all layers except the last one\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.at_model.mlp_layer = nn.Identity()\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T10:10:10.706387131Z",
     "start_time": "2023-08-20T10:10:10.221230032Z"
    }
   },
   "id": "ba4f7f39afdc92da"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zivk/Documents/Deep Learning/whisper-at/venv/lib/python3.10/site-packages/whisper_at/transcribe.py:121: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "last_layer_size = model.at_model.layer_tr.mlp_ln.weight.shape[0]\n",
    "result = model.transcribe(\"/home/zivk/Documents/Deep Learning/Datasets/ESC-50-master/audio/1-137-A-32.wav\", at_time_res=audio_tagging_time_resolution, mlp_ln_output=last_layer_size)\n",
    "# ASR Results\n",
    "# print(result[\"text\"])\n",
    "print(result[\"audio_tag\"].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T10:10:26.446595527Z",
     "start_time": "2023-08-20T10:10:24.567858015Z"
    }
   },
   "id": "404818ef6ce9e76a"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 32",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 8\u001B[0m\n\u001B[1;32m      4\u001B[0m train_labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m32\u001B[39m])\n\u001B[1;32m      6\u001B[0m classifier \u001B[38;5;241m=\u001B[39m LogisticRegression(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, C\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.316\u001B[39m, max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, multi_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124movr\u001B[39m\u001B[38;5;124m\"\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m \u001B[43mclassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Evaluate using the logistic regression classifier\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# predictions = classifier.predict( val_features)\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# accuracy = accuracy_score(val_labels, predictions)\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Deep Learning/whisper-at/venv/lib/python3.10/site-packages/sklearn/base.py:1151\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1144\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1147\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1148\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1149\u001B[0m     )\n\u001B[1;32m   1150\u001B[0m ):\n\u001B[0;32m-> 1151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Deep Learning/whisper-at/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1252\u001B[0m, in \u001B[0;36mLogisticRegression.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m   1250\u001B[0m classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_\n\u001B[1;32m   1251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_classes \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m-> 1252\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1253\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis solver needs samples of at least 2 classes\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1254\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in the data, but the data contains only one\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1255\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m class: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1256\u001B[0m         \u001B[38;5;241m%\u001B[39m classes_[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1257\u001B[0m     )\n\u001B[1;32m   1259\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m   1260\u001B[0m     n_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mValueError\u001B[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 32"
     ]
    }
   ],
   "source": [
    "train_features = result[\"audio_tag\"]\n",
    "\n",
    "train_labels = torch.tensor([32])\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, multi_class=\"ovr\", verbose=1)\n",
    "\n",
    "classifier.fit(train_features, train_labels)\n",
    "# Evaluate using the logistic regression classifier\n",
    "# predictions = classifier.predict( val_features)\n",
    "\n",
    "# accuracy = accuracy_score(val_labels, predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T10:41:57.123441476Z",
     "start_time": "2023-08-20T10:41:57.033044162Z"
    }
   },
   "id": "2fe21fa9ffaeff24"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "train_labels = torch.tensor([32])\n",
    "\n",
    "print(train_labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T10:11:18.542086996Z",
     "start_time": "2023-08-20T10:11:18.505730469Z"
    }
   },
   "id": "477950d9b6a6d370"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
